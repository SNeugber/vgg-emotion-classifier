{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../data')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"../data\")\n",
    "data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all WAV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../data/03-01-01-01-01-01-01.wav'),\n",
       " WindowsPath('../data/03-01-01-01-01-01-02.wav'),\n",
       " WindowsPath('../data/03-01-01-01-01-01-03.wav'),\n",
       " WindowsPath('../data/03-01-01-01-01-01-04.wav'),\n",
       " WindowsPath('../data/03-01-01-01-01-01-05.wav')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_files = data_path.glob(\"*.wav\")\n",
    "wav_files = list(wav_files)\n",
    "wav_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map file names to their classes.\n",
    "\n",
    "Each emotion is labelled as 01 - 08, so we convert that to labels 0 - 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('03-01-01-01-01-01-01.wav', 0),\n",
       " ('03-01-01-01-01-01-02.wav', 0),\n",
       " ('03-01-01-01-01-01-03.wav', 0),\n",
       " ('03-01-01-01-01-01-04.wav', 0),\n",
       " ('03-01-01-01-01-01-05.wav', 0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def class_from_file_name(fname):\n",
    "    return int(fname.split('-')[2]) - 1\n",
    "\n",
    "labels = {\n",
    "    f.name: class_from_file_name(f.name)\n",
    "    for f in wav_files\n",
    "}\n",
    "[(k, v) for k, v in labels.items()][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(set(labels.values()))\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix PYTHONPATH\n",
    "\n",
    "Add the path to the vgg-related files to the pythonpath so that we can import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/Work/playground/vgg-emotion-classifier/vgg')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_dir = Path(os.getcwd())\n",
    "vgg_dir = nb_dir.parent / 'vgg'\n",
    "vgg_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'C:\\\\Users\\\\Sam\\\\Anaconda3\\\\envs\\\\vggec\\\\python36.zip',\n",
       " 'C:\\\\Users\\\\Sam\\\\Anaconda3\\\\envs\\\\vggec\\\\DLLs',\n",
       " 'C:\\\\Users\\\\Sam\\\\Anaconda3\\\\envs\\\\vggec\\\\lib',\n",
       " 'C:\\\\Users\\\\Sam\\\\Anaconda3\\\\envs\\\\vggec',\n",
       " 'C:\\\\Users\\\\Sam\\\\Anaconda3\\\\envs\\\\vggec\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\Sam\\\\Anaconda3\\\\envs\\\\vggec\\\\lib\\\\site-packages\\\\IPython\\\\extensions',\n",
       " 'C:\\\\Users\\\\Sam\\\\.ipython',\n",
       " 'D:\\\\Work\\\\playground\\\\vgg-emotion-classifier\\\\vgg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if str(vgg_dir) not in sys.path:\n",
    "    sys.path.append(str(vgg_dir))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read WAV files\n",
    "\n",
    "Read in the wav files and convert them into the correct shape for the VGGish model (this is thankfully taken care of already by the example code provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vggish_input import wavfile_to_examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    f.name: wavfile_to_examples(str(f))\n",
    "    for f in wav_files\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 96, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['03-01-01-01-01-01-01.wav'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset\n",
    "\n",
    "Split into\n",
    "\n",
    "* train: 70%\n",
    "* val: 15%\n",
    "* test: 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 1008\n",
      "Validation size: 216\n",
      "Training size: 216\n"
     ]
    }
   ],
   "source": [
    "seed = 987234871\n",
    "\n",
    "x_train_keys, x_test_keys, y_train, y_test = train_test_split(list(labels.keys()),\n",
    "                                                    list(labels.values()),\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = seed)\n",
    "\n",
    "x_train_keys, x_val_keys, y_train, y_val = train_test_split(x_train,\n",
    "                                                            y_train,\n",
    "                                                            test_size = 0.15 / (1 - 0.15),\n",
    "                                                            random_state = seed)\n",
    "\n",
    "print(f\"Training size: {len(x_train_keys)}\")\n",
    "print(f\"Validation size: {len(x_val_keys)}\")\n",
    "print(f\"Training size: {len(x_test_keys)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../pretrained_models/vggish_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = '../pretrained_models/vggish_model.ckpt'\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    \n",
    "    embeddings = vggish_slim.define_vggish_slim(training=True)\n",
    "\n",
    "    # Define a shallow classification model and associated training ops on top\n",
    "    # of VGGish.\n",
    "    with tf.variable_scope('emo-classifier'):\n",
    "        # Add a fully connected layer with 100 units.\n",
    "        num_units = 100\n",
    "    \n",
    "        \n",
    "        seq = keras.layers.CuDNNLSTM(units, return_sequences=True)(embeddings)\n",
    "        fc = slim.fully_connected(seq, num_units)\n",
    "\n",
    "        # Add a classifier layer at the end, consisting of parallel logistic\n",
    "        # classifiers, one per class. This allows for multi-class tasks.\n",
    "        logits = slim.fully_connected(\n",
    "            fc, NUM_CLASSES, activation_fn=None, scope='logits')\n",
    "        tf.sigmoid(logits, name='prediction')\n",
    "\n",
    "        # Add training ops.\n",
    "        with tf.variable_scope('train'):\n",
    "            global_step = tf.Variable(\n",
    "                0, name='global_step', trainable=False,\n",
    "                collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                             tf.GraphKeys.GLOBAL_STEP])\n",
    "\n",
    "            # Labels are assumed to be fed as a batch multi-hot vectors, with\n",
    "            # a 1 in the position of each positive class label, and 0 elsewhere.\n",
    "            labels = tf.placeholder(\n",
    "                tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n",
    "\n",
    "            # Cross-entropy label loss.\n",
    "            xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=logits, labels=labels, name='xent')\n",
    "            loss = tf.reduce_mean(xent, name='loss_op')\n",
    "            tf.summary.scalar('loss', loss)\n",
    "\n",
    "            # We use the same optimizer and hyperparameters as used to train VGGish.\n",
    "            optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=vggish_params.LEARNING_RATE,\n",
    "                epsilon=vggish_params.ADAM_EPSILON)\n",
    "            optimizer.minimize(loss, global_step=global_step, name='train_op')\n",
    "\n",
    "    # Initialize all variables in the model, and then load the pre-trained\n",
    "    # VGGish checkpoint.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n",
    "\n",
    "    # Locate all the tensors and ops we need for the training loop.\n",
    "    features_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.INPUT_TENSOR_NAME)\n",
    "    labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')\n",
    "    global_step_tensor = sess.graph.get_tensor_by_name(\n",
    "        'mymodel/train/global_step:0')\n",
    "    loss_tensor = sess.graph.get_tensor_by_name('mymodel/train/loss_op:0')\n",
    "    train_op = sess.graph.get_operation_by_name('mymodel/train/train_op')\n",
    "\n",
    "    # The training loop.\n",
    "    for _ in range(FLAGS.num_batches):\n",
    "      (features, labels) = _get_examples_batch()\n",
    "      [num_steps, loss, _] = sess.run(\n",
    "          [global_step_tensor, loss_tensor, train_op],\n",
    "          feed_dict={features_tensor: features, labels_tensor: labels})\n",
    "      print('Step %d: loss %g' % (num_steps, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
